[1](https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/)
[2](https://explodingtopics.com/blog/chatgpt-users)
[3](https://berkeleyrdi.substack.com/p/agentic-ai-weekly-berkeley-rdi-september-0b9)
[4](https://aws.amazon.com/what-is/reinforcement-learning-from-human-feedback/)
[5](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)
[6](https://moonshotai.github.io/Kimi-Researcher/)
[7](https://arxiv.org/abs/2501.12948)
[8](https://huggingface.co/papers/2501.12948)
[9](https://www.youtube.com/watch?v=r1qZpYAmqmg)
[10](https://rdi.berkeley.edu/agentic-ai/f25)
[11](https://radicaldatascience.wordpress.com)
[12](https://www.deeplearning.ai)
[13](https://www.linkedin.com/posts/ishan-jayswal-800a8619_ai-llm-machinelearning-activity-7373649799645904896-GgaL)
[14](https://arxiv.org/html/2501.12948v1)
[15](https://www.reddit.com/r/machinelearningnews/comments/1lj5unf/moonshot_ai_unveils_kimiresearcher_an/)
[16](https://dataglobalhub.org/resource/articles/moonshot-a-is-kimi-researcher-advancing-autonomous-ai-for-research)
[17](https://www.youtube.com/watch?v=oUktM2jLM6E)
[18](https://berkeleyrdi.substack.com/p/agentic-ai-weekly-berkeley-rdi-october-954)
[19](https://huggingface.co/deepseek-ai/DeepSeek-R1)
[20](https://www.marktechpost.com/2025/06/24/moonshot-ai-unveils-kimi-researcher-an-reinforcement-learning-rl-trained-agent-for-complex-reasoning-and-web-scale-search/)
[21](https://rdi.berkeley.edu)
[22](https://www.youtube.com/watch?v=DCqqCLlsIBU)
[23](https://rysysthtechnologies.com/insights/meet-kimi-researcher-a-different-kind-of-ai-agent)
[24](https://www.linkedin.com/posts/arhaanaggarwal_we-were-fortunate-to-welcomeyann-dubois-activity-7373847395576623104-vsqk)
[25](https://techcrunch.com/2023/11/06/openais-chatgpt-now-has-100-million-weekly-active-users/)
[26](https://www.businessinsider.com/chatgpt-users-growth-openai-growth-sam-altman-ai-llm-2025-10)
[27](https://www.hindustantimes.com/technology/chatgpt-sets-record-for-fastest-growing-app-with-100-million-users-report-101675323297627.html)
[28](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up)
[29](https://www.coursera.org/articles/rlhf)
[30](https://en.wikipedia.org/wiki/ChatGPT)
[31](https://ravinkumar.com/GenAiGuidebook/deepdive/llama2.html)
[32](https://www.superannotate.com/blog/rlhf-for-llm)
[33](https://www.forbes.com/sites/martineparis/2023/02/03/chatgpt-hits-100-million-microsoft-unleashes-ai-bots-and-catgpt-goes-viral/)
[34](https://www.llama.com/llama2/)
[35](https://techcrunch.com/2025/10/17/chatgpts-mobile-app-is-seeing-slowing-download-growth-and-daily-use-analysis-shows/)
[36](https://www.datacamp.com/tutorial/fine-tuning-llama-2)
[37](https://huggingface.co/blog/rlhf)
[38](https://www.theguardian.com/technology/2023/feb/02/chatgpt-100-million-users-open-ai-fastest-growing-app)
[39](https://huyenchip.com/2023/05/02/rlhf.html)
[40](https://backlinko.com/chatgpt-stats)

[1](https://dl.acm.org/doi/10.1145/3458817.3476209)
[2](https://arxiv.org/pdf/2204.02311.pdf)
[3](https://arxiv.org/abs/2205.14135)
[4](https://www.deepspeed.ai/tutorials/zero/)
[5](https://arxiv.org/abs/1909.08053)
[6](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html)
[7](https://docs.nvidia.com/physicsnemo/latest/user-guide/performance_docs/torch_compile_support.html)
[8](https://docs.nvidia.com/deeplearning/performance/pdf/Training-Mixed-Precision-User-Guide.pdf)
[9](https://pytorch.org/blog/what-every-user-should-know-about-mixed-precision-training-in-pytorch/)
[10](https://docs.pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)
[11](https://arxiv.org/abs/2307.08691)
[12](https://tridao.me/publications/flash3/flash3.pdf)
[13](https://www.glennklockwood.com/garden/LLM-training)
[14](https://fid3024.github.io/papers/2019%20-%20GPipe:%20Efficient%20Training%20of%20Giant%20Neural%20Networks%20using%20Pipeline%20Parallelism.pdf)
[15](https://arxiv.org/pdf/2101.03961.pdf)
[16](https://huggingface.co/blog/kv-cache-quantization)
[17](https://github.com/Dao-AILab/flash-attention)
[18](https://www.semanticscholar.org/paper/FlashAttention:-Fast-and-Memory-Efficient-Exact-Dao-Fu/87c5b281fa43e6f27191b20a8dd694eda1126336)
[19](https://apxml.com/courses/how-to-build-a-large-language-model/chapter-16-implementing-distributed-training-frameworks/using-deepspeed-zero-optimizations)
[20](https://www.abhik.xyz/articles/compiling-pytorch-kernel)
[21](https://tridao.me/publications/flash2/flash2.pdf)
[22](https://www.deepspeed.ai/tutorials/zero-offload/)
[23](https://www.latent.space/p/flashattention)
[24](https://deepspeed.readthedocs.io/en/latest/zero3.html)
[25](https://discuss.pytorch.org/t/fusing-operators-in-torch-compile-for-codegen/207956)
[26](https://openreview.net/pdf?id=H4DqfPSibmx)
[27](https://huggingface.co/docs/transformers/en/deepspeed)
[28](https://aws.amazon.com/blogs/machine-learning/accelerated-pytorch-inference-with-torch-compile-on-aws-graviton-processors/)
[29](https://openreview.net/forum?id=mZn2Xyh9Ec)
[30](https://arxiv.org/pdf/1909.08053.pdf)
[31](https://github.com/NVIDIA/Megatron-LM)
[32](https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf)
[33](https://huggingface.co/blog/moe)
[34](https://www.gaohongnan.com/playbook/training/how_to_calculate_flops_in_transformer_based_models.html)
[35](https://www.usenix.org/system/files/atc24-yuan.pdf)
[36](https://machinelearningmastery.com/mixture-of-experts-architecture-in-transformer-models/)
[37](https://www.glennklockwood.com/garden/MFU)
[38](https://huggingface.co/google/switch-base-16)
[39](https://developer.nvidia.com/megatron-core)
[40](https://openreview.net/pdf?id=80SSl69GAz)
[41](https://www.adamcasson.com/posts/transformer-flops)
[42](https://www.semanticscholar.org/paper/Megatron-LM:-Training-Multi-Billion-Parameter-Using-Shoeybi-Patwary/8323c591e119eb09b28b29fd6c7bc76bd889df7a)
[43](https://services.google.com/fh/files/blogs/tpu_v4_benchmarking.pdf)
[44](https://huggingface.co/docs/accelerate/en/usage_guides/megatron_lm)
[45](https://www.ibm.com/think/topics/mixture-of-experts)
[46](https://www.runpod.io/articles/guides/fp16-bf16-fp8-mixed-precision-speed-up-my-model-training)
[47](https://docs.fast.ai/callback.fp16.html)
[48](https://arxiv.org/html/2505.22922v1)
[49](https://developer.nvidia.com/automatic-mixed-precision)
[50](https://arxiv.org/pdf/1811.06965.pdf)
[51](https://apxml.com/courses/how-to-build-a-large-language-model/chapter-20-mixed-precision-training-techniques/using-bf16-bfloat16-format)
[52](https://papers.nips.cc/paper/8305-gpipe-efficient-training-of-giant-neural-networks-using-pipeline-parallelism)
[53](https://huggingface.co/docs/transformers/v4.17.0/en/performance)
[54](https://www.dgl.ai/dgl_docs/guide/mixed_precision.html)
[55](https://proceedings.neurips.cc/paper/2019/file/093f65e080a295f8076b1c5722a46aa2-Reviews.html)
[56](https://huggingface.co/docs/transformers/v4.38.2/perf_train_gpu_one)
[57](https://www.cs.toronto.edu/ecosystem/documents/AMP-Tutorial.pdf)
[58](https://www.sciencedirect.com/science/article/pii/S0167739X23001735)
[59](https://www.alibabacloud.com/help/en/pai/getting-started/estimation-of-the-required-video-memory-for-the-model)
[60](https://stackoverflow.com/questions/72288020/how-to-enable-mixed-precision-training)

[1](https://en.wikipedia.org/wiki/MMLU)
[2](https://arxiv.org/html/2403.04132v1)
[3](https://allenai.org/blog/fluid-benchmarking)
[4](https://deepeval.com/docs/benchmarks-mmlu)
[5](https://www.enkefalos.com/evaluating-large-language-models-llm-benchmarks/)
[6](https://aclanthology.org/2025.acl-long.656/)
[7](https://github.com/microsoft/MMLU-CF)
[8](https://lmsys.org/blog/2023-05-03-arena/)
[9](https://arstechnica.com/ai/2023/12/turing-test-on-steroids-chatbot-arena-crowdsources-ratings-for-45-ai-models/)
[10](https://github.com/tatsu-lab/alpaca_eval/blob/main/README.md)
[11](https://github.com/tatsu-lab/alpaca_eval)
[12](https://web.stanford.edu/class/cs224n/slides/cs224n-spr2024-lecture11-evaluation-yann.pdf)
[13](https://arxiv.org/html/2504.10045v1)
[14](https://openreview.net/pdf?id=ODibPQmeP1)
[15](https://www.projectpro.io/article/mmlu-benchmark/1162)
[16](https://www.accuwebhosting.com/blog/mmlu-benchmark/)
[17](https://arxiv.org/html/2501.02189v1)
[18](https://deepgram.com/learn/mmlu-llm-benchmark-guide)
[19](https://openlm.ai/chatbot-arena/)
[20](https://www.confident-ai.com/blog/llm-arena-as-a-judge-llm-evals-for-comparison-based-testing)

[1](https://berkeleyrdi.substack.com/p/agentic-ai-weekly-berkeley-rdi-september-0b9)
[2](https://pytorch.org/blog/a-primer-on-llm-post-training/)
[3](https://arxiv.org/abs/2501.12948)
[4](https://labs.adaline.ai/p/inside-reasoning-models-openai-o3)
[5](https://arxiv.org/html/2308.10792v9)
[6](https://github.com/tatsu-lab/stanford_alpaca)
[7](https://crfm.stanford.edu/2023/03/13/alpaca.html)
[8](https://arxiv.org/html/2507.20534v1)
[9](https://www.digitalocean.com/community/tutorials/post-training-agentic-models-kimi-k2)
[10](https://aws.amazon.com/blogs/machine-learning/fine-tune-large-language-models-with-reinforcement-learning-from-human-or-ai-feedback/)
[11](https://huggingface.co/learn/llm-course/en/chapter12/3)
[12](https://arxiv.org/abs/2305.11206)
[13](https://aclanthology.org/2023.findings-emnlp.1011.pdf)
[14](https://papers.nips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html)
[15](https://www.youtube.com/watch?v=QdEuh2UVbu0)
[16](https://www.reddit.com/r/LocalLLaMA/comments/1i8rujw/notes_on_deepseek_r1_just_how_good_it_is_compared/)
[17](https://ghost.oxen.ai/why-grpo-is-important-and-how-it-works/)
[18](https://www.philschmid.de/deepseek-r1)
[19](https://composio.dev/blog/notes-on-kimi-k2)
[20](https://arxiv.org/abs/2503.20783)
[21](https://huggingface.co/blog/NormalUhr/grpo)
[22](https://www.kimi.com/artifact-preview/1980c532-5631-8c55-885f-517d380005e7)
[23](https://arxiv.org/pdf/2308.14186.pdf)
[24](https://www.reddit.com/r/singularity/comments/1i5uc3t/a_summary_of_deepseekr1s_paper_by_deepseekr1/)
[25](https://www.baseten.co/blog/kimi-k2-explained-the-1-trillion-parameter-model-redefining-how-to-build-agents/)
[26](https://www.youtube.com/watch?v=YCawyzAOg1Y)
[27](https://arxiv.org/pdf/2305.11206.pdf)
[28](https://arxiv.org/pdf/2307.12057.pdf)
[29](https://www.semanticscholar.org/paper/LIMA:-Less-Is-More-for-Alignment-Zhou-Liu/546d0624adfc6e18fb87d8cc77e7705bb9ea7445)
[30](https://arxiv.org/html/2502.03387v1)
[31](https://www.lightly.ai/blog/rlhf-reinforcement-learning-from-human-feedback)
[32](https://howiehwong.github.io/preference_leakage.pdf)
[33](https://huggingface.co/papers/2305.11206)
[34](https://arxiv.org/html/2404.18922v1)
[35](https://huggingface.co/papers/2502.01534)
[36](https://www.reddit.com/r/LocalLLaMA/comments/13oijm0/lima_less_is_more_for_alignment/)
[37](https://huggingface.co/blog/NormalUhr/rlhf-pipeline)
[38](https://arxiv.org/html/2410.21819v1)
[39](https://neurips.cc/virtual/2023/poster/72022)
[40](http://arxiv.org/abs/2305.11206)
[41](https://aws.amazon.com/blogs/machine-learning/align-meta-llama-3-to-human-preferences-with-dpo-amazon-sagemaker-studio-and-amazon-sagemaker-ground-truth/)
[42](https://arxiv.org/html/2412.05579v2)
[43](https://openreview.net/forum?id=KBMOKmX2he)

[1](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1)
[2](https://arxiv.org/abs/2406.17557)
[3](https://huggingface.co/datasets/HuggingFaceFW/fineweb)
[4](https://neurips.cc/virtual/2024/poster/97513)
[5](https://www.reddit.com/r/LocalLLaMA/comments/1d6zwgy/fineweb_decanting_the_web_for_the_finest_text/)
[6](https://arxiv.org/html/2404.17785v4)
[7](https://www.educatingsilicon.com/2024/04/29/revised-chinchilla-scaling-laws-impact-on-llm-compute-and-token-requirements/)
[8](https://openreview.net/forum?id=n6SCkn2QaG)
[9](https://arxiv.org/abs/2004.10802)
[10](https://lifearchitect.ai/chinchilla/)
[11](https://news.ycombinator.com/item?id=40552548)
[12](https://arxiv.org/abs/2001.08361)
[13](https://legalgenie.com.au/artificial-intelligence/chinchilla-point/)
[14](https://huggingface.co/datasets/jhu-clsp/mmBERT-pretrain-p2-fineweb2-remaining)
[15](https://arxiv.org/abs/2010.14701)
[16](https://arxiv.org/html/2404.10102v1)
[17](https://www.reddit.com/r/MachineLearning/comments/1d68jjf/r_tech_report_on_fineweb_decanting_the_web_for/)
[18](https://arxiv.org/pdf/2001.08361.pdf)
[19](https://epoch.ai/blog/chinchilla-scaling-a-replication-attempt)
[20](http://arxiv.org/abs/2406.17557v2)
